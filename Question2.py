# -*- coding: utf-8 -*-
"""Copy of Untitled17.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13FFr3_fA_iF7a0QkUuFz7gsTKdfqXCYm
"""

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

from sklearn.neighbors import KernelDensity
import math
import multiprocessing 
#The Haversine formula determines the distance between two places based on the central angle 
#produced between the two points and the centre of the Earth, taking into account the curvature of the Earth's surface. 
def haversine(lat1, lon1, alt1, lat2, lon2, alt2):
    R = 6371e3 # radius of the Earth in meters
    phi1 = math.radians(lat1)
    phi2 = math.radians(lat2)
    delta_phi = math.radians(lat2 - lat1)
    delta_lambda = math.radians(lon2 - lon1)

    a = math.sin(delta_phi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(delta_lambda/2)**2
    c = 2*math.atan2(math.sqrt(a), math.sqrt(1-a))

    d = R*c + (alt2 - alt1)

    return d

import pandas as pd
import multiprocessing

def process_individual(df_individual):
    total_distance = 0

    # iterate over rows and calculate distances because we want to know the total distance by each user in the dataset.
    for i in range(1, len(df_individual)):
        lat1, lon1, alt1 = df_individual.iloc[i-1][['latitude', 'longitude', 'altitude']]
        lat2, lon2, alt2 = df_individual.iloc[i][['latitude', 'longitude', 'altitude']]
        total_distance += haversine(lat1, lon1, alt1, lat2, lon2, alt2)

    return total_distance
#This is a function that calculates the total distance traveled by each individual in the DataFrame
# in parallel using the multiprocessing library

def calculate_total_distance_parallel(df):
    # group the DataFrame by individual_id as it would give us the ease to seperate the rows with similar individual_id
    groups = df.groupby('individual_id')

    pool = multiprocessing.Pool()

    # map the function to each group and calculate the sum of distances in parallel
    results = pool.map(process_individual, [group[1] for group in groups if 'individual_id' in group[1]])

    # close the pool and wait for all processes to finish
    pool.close()
    pool.join()

    # create dictionary with total distances
    total_distances = {name: distance for name, distance in zip(groups.groups.keys(), results)}

    return total_distances

df = pd.read_csv('/content/drive/MyDrive/hi/combined_trajectories.csv')

len(df)

unique_ids = df['individual_id'].unique()

new_df = pd.DataFrame()

for uid in unique_ids:
    temp_df = df[df['individual_id'] == uid].head(500)
    new_df = new_df.append(temp_df)

calculate_total_distance_parallel(new_df)

#created Numpy array as coords containing the value of latitute and longitute so that it can be used for working with the geospatial data
# the values are scaled so that the coordinates have zero mean and unit variance
#this could can help to remove biases introduced by variations in the scale 

#created a DBSCAN object with a distance epsilon of 100 meters and a minimum cluster size of 5
#DBSCAN is a clustering algorithm that groups together data points that are close to each other based on their spatial proximity

# calculated the silhouette score 
#this step is essential to measures how similar an object is to its own cluster compared to other clusters
# centroid of the data to center the map
# created a map centered at the centroid
# created a color map for the clusters (noise points are black and colors list is used to define the colors that will be assigned to the non-noise clusters)

import pandas as pd
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import folium
from google.colab import files

def cluster_and_map(individual_id, df_group):
    coords = df_group[['latitude', 'longitude']].values
    scaler = StandardScaler()
    coords_scaled = scaler.fit_transform(coords)

    dbscan = DBSCAN(eps=0.0009, min_samples=5)
    dbscan.fit(coords_scaled)
    labels = dbscan.labels_

    if len(np.unique(labels)) > 1: 
        silhouette = silhouette_score(coords_scaled, labels)
        print(f"Silhouette score for individual_id {individual_id}: {silhouette}")
    else:
        print("No non-noise clusters found")

    df_group['cluster'] = labels

    centroid = np.mean(coords, axis=0)
    m = folium.Map(location=centroid, zoom_start=12)

    color_map = {-1: 'black'}
    colors = ['blue', 'green', 'purple', 'orange', 'pink', 'red', 'gray', 'beige']
    for i, label in enumerate(np.unique(labels)):
        if label != -1:
            color_map[label] = colors[i % len(colors)]

    for i, row in df_group.iterrows():
        folium.CircleMarker(location=[row['latitude'], row['longitude']],radius=5,color=color_map[row['cluster']],fill=True,fill_color=color_map[row['cluster']],fill_opacity=0.7,
        ).add_to(m)

    m.save(f'{individual_id}_map.html')
    files.download(f'{individual_id}_map.html')

df

for individual_id, df_group in df.groupby('individual_id'):
    
    df_group = df_group.sample(n=7000, random_state=1, replace=True) 
    cluster_and_map(individual_id, df_group)

from google.colab import drive
drive.mount('/content/drive')




# extract latitude and longitude values
X = df[['latitude', 'longitude']].values


if len(X) > 300000:
    idx = np.random.choice(len(X), size=300000, replace=False)
    X = X[idx]

# perform KDE with a bandwidth of 0.05
kde = KernelDensity(bandwidth=0.05, metric='haversine', kernel='gaussian')
kde.fit(np.radians(X))

# generate a grid of points to evaluate the KDE at
xmin, ymin, xmax, ymax = df[['longitude', 'latitude']].agg([np.min, np.max]).values.ravel()
xgrid, ygrid = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]
xy = np.column_stack([xgrid.ravel(), ygrid.ravel()])

# evaluate the KDE at the grid points
log_density = kde.score_samples(np.radians(xy)).reshape(xgrid.shape)

# plot the KDE as a heatmap and save the plot as a PNG file
fig, ax = plt.subplots()
ax.imshow(np.exp(log_density), origin='lower', extent=[xmin, xmax, ymin, ymax], cmap='inferno')
ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')
ax.set_title('Kernel Density Estimation (KDE)')
plt.savefig('kde_plot.png')

